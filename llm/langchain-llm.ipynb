{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大模型示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LLM大模型\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xinference方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 巴黎、马赛、里昂、波尔多、巴黎圣母院、凡尔赛宫、勒阿弗莱、马赛、尼斯 B: 巴黎、蒙彼利埃、图卢兹、巴黎圣母院、斯特拉斯堡、里昂、波尔多、巴黎罗浮宫、尼斯 C: 巴黎、里昂、马赛、波尔多、巴黎圣母院、凡尔赛宫、勒阿弗莱、摩纳哥、马赛 D: 巴黎、马赛、里昂、波尔多、图卢兹、巴黎圣母院、斯特拉斯堡、里昂、尼斯 Q: 根据所给选项，下列哪项是正确的选项? A: 巴黎 B: 蒙彼利埃 C: 图卢兹 D: 马赛 Q: Which of the following is correct according to the given options? A: Paris B: Montpellier C: Toulouse D: Marseille\n",
      "A: 巴黎是法国的主要旅游城市和经济中心之一，拥有众多著名景点如埃菲尔铁塔、卢浮宫、圣母院等。因此，正确答案是A: 巴黎。"
     ]
    }
   ],
   "source": [
    "# xinference本地化部署\n",
    "# author：sizhong du\n",
    "# since：2025-03-18\n",
    "\n",
    "# 安装依赖\n",
    "# pip install xinference_client\n",
    "# 启动xinference\n",
    "# $ xinference\n",
    "\n",
    "from langchain_community.llms import Xinference\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = Xinference(\n",
    "    server_url=\"http://192.168.31.5:9997\",\n",
    "    model_uid=\"qwen1.5-chat\",\n",
    "    stream=True\n",
    ")\n",
    "prompt = PromptTemplate(\n",
    "    input=['country'],\n",
    "    template=\"Q: 列举{country}的十大城市名? A:\"\n",
    ")\n",
    "chain = prompt | llm\n",
    "for chunk in chain.stream(input={'country': '法国'}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ollama方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\n\\n</think>\\n\\n您好！我是由中国的深度求索（DeepSeek）公司开发的智能助手DeepSeek-R1。如您有任何任何问题，我会尽我所能为您提供帮助。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 6, 'total_tokens': 46, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek-r1:14b', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None} id='run-be86864e-f45a-44c3-9613-02b36aaad385-0' usage_metadata={'input_tokens': 6, 'output_tokens': 40, 'total_tokens': 46, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# ollama本地化部署\n",
    "# 2025-02-28\n",
    "\n",
    "# pip install langchain\n",
    "# pip install langchain-ollama\n",
    "\n",
    "\"\"\"\n",
    "# 下载模型\n",
    "ollama pull qwen2.5:1.5b\n",
    "ollama pull bge-m3\n",
    "\n",
    "# 启动模型\n",
    "ollama list\n",
    "ollama run qwen2.5:1.5b\n",
    "访问：http://localhost:11434\n",
    "访问：http://localhost:11434/api/tags\n",
    "\"\"\"\n",
    "\n",
    "# openai兼容方式\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    # model = \"qwen2.5:7b\",\n",
    "    model = \"deepseek-r1:14b\",\n",
    "    base_url = \"http://192.168.31.5:11434/v1\",\n",
    "    api_key = \"ollama\",\n",
    "    temperature = 0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# ollama方式\n",
    "# from langchain_ollama import ChatOllama\n",
    "# llm = ChatOllama(base_url=\"http://localhost:11434\", model=\"qwen2.5:1.5b\")\n",
    "\n",
    "result = llm.invoke(\"你是谁？\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### langchain openai云服务方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Bonjour, comment puis-je vous appeler ?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 34, 'total_tokens': 44, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-7b-instruct-1m', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-da4384f8-c666-48a5-aa4e-e89aa9aa8eeb-0' usage_metadata={'input_tokens': 34, 'output_tokens': 10, 'total_tokens': 44, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# laingchain openai\n",
    "# 非本地化部署，调用兼容openai大模型api实现\n",
    "# 2025-02-28\n",
    "\n",
    "\"\"\"\n",
    "安装依赖\n",
    "conda create -n langchain python=3.10\n",
    "pip install langchain\n",
    "pip install langchain-community\n",
    "pip install langchain-openai\n",
    "\"\"\"\n",
    "\n",
    "# import os\n",
    "# import getpass\n",
    "\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# \n",
    "llm = ChatOpenAI(\n",
    "    model = \"qwen2.5-7b-instruct-1m\",\n",
    "    base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    api_key = \"sk-064f1364f5b34904a425f22613347cf5\",\n",
    "    temperature = 0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"你是谁？\"}]\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful assistant that translates English to French. Translate the user sentence.\"),\n",
    "    (\"human\", \"你好，怎么称呼？\"),\n",
    "]\n",
    "\n",
    "result = llm.invoke(messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openai标准方式\n",
    "\n",
    "**并使用langsmith进行链路跟踪**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_value\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None)\n",
      "ChoiceDelta(content='我是', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='来自', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='阿里', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='云', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='的大', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='规模', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='语言', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='模型', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='，', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='我', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='叫', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='通', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='义', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='千', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='问', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='。', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "ChoiceDelta(content='', function_call=None, refusal=None, role=None, tool_calls=None)\n",
      "CompletionUsage(completion_tokens=17, prompt_tokens=21, total_tokens=38, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=17), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=None, text_tokens=21))\n"
     ]
    }
   ],
   "source": [
    "# openai标准调用方式示例\n",
    "# author: sizhong du\n",
    "# since: 2025-02-24\n",
    "\n",
    "\n",
    "# langsmith进行链路跟踪\n",
    "# 1. 在https://smith.langchain.com/ 注册账号\n",
    "# 2. 在网站创建项目并生成api-key\n",
    "# 3. 设置以下环境变量\n",
    "import os\n",
    "os.environ['LANGSMITH_TRACING']=\"true\"\n",
    "os.environ['LANGSMITH_ENDPOINT']=\"https://api.smith.langchain.com\"\n",
    "os.environ['LANGSMITH_API_KEY']=\"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\"\n",
    "os.environ['LANGSMITH_PROJECT']=\"pr-potable-experiment-71\"\n",
    "print(os.getenv('LANGCHAIN_PROJECT', 'default_value'))\n",
    "\n",
    "# 4. 导入wrap包\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "from openai import OpenAI\n",
    "# 5. 包装OpenAI以进行链路跟踪，可在smith.langchain.com查看调用跟踪记录\n",
    "client = wrap_openai(OpenAI(\n",
    "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "    # api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    api_key = \"sk-a5c5eb662fa64bb0b50e25765808d9f1\",\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "))\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen-omni-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"你是谁\"}],\n",
    "    # 设置输出数据的模态，当前支持两种：[\"text\",\"audio\"]、[\"text\"]\n",
    "    # modalities=[\"text\", \"audio\"],\n",
    "    # audio={\"voice\": \"Cherry\", \"format\": \"wav\"},\n",
    "    # stream 必须设置为 True，否则会报错\n",
    "    stream=True,\n",
    "    stream_options={\"include_usage\": True},\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    if chunk.choices:\n",
    "        print(chunk.choices[0].delta)\n",
    "    else:\n",
    "        print(chunk.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. embedding模型\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天地灵气孕育出一颗能量巨大的混元珠，元始天尊将混元珠提炼成灵珠和魔丸，灵珠投胎为人，助周伐纣时可堪大用；而魔丸则会诞出魔王，为祸人间。元始天尊启动了天劫咒语，3年后天雷将会降临，摧毁魔丸。太乙受命将灵珠托生于陈塘关李靖家的儿子哪吒身上。然而阴差阳错，灵珠和魔丸竟然被掉包。本应是灵珠英雄的哪吒却成了混世大魔王。调皮捣蛋顽劣不堪的哪吒却徒有一颗做英雄的心。然而面对众人对魔丸的误解和即将来临的天雷的降临，哪吒是否命中注定会立地成魔？他将何去何从？\n"
     ]
    }
   ],
   "source": [
    "# Embedding模型示例\n",
    "# author: sizhong du\n",
    "# since: 2025-03-18\n",
    "\n",
    "\n",
    "# 百炼云服务方式\n",
    "# pip install langchain-community\n",
    "# pip install dashscope\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v2\",\n",
    "    dashscope_api_key=\"sk-a5c5eb662fa64bb0b50e25765808d9f1\"\n",
    ")\n",
    "\n",
    "# ollama方式\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3:latest\",\n",
    ")\n",
    "\n",
    "# xinference方式\n",
    "from langchain_community.embeddings import XinferenceEmbeddings\n",
    "embeddings2 = XinferenceEmbeddings(\n",
    "    server_url=\"http://192.168.31.5:9997\",\n",
    "    model_uid=\"bce-embedding-base_v1\",\n",
    ")\n",
    "\n",
    "\n",
    "# text = \"hello world\"\n",
    "# single_vector = embeddings.embed_query(text)\n",
    "# print(single_vector)\n",
    "\n",
    "# 使用向量数据库\n",
    "text_1 = \"天地灵气孕育出一颗能量巨大的混元珠，元始天尊将混元珠提炼成灵珠和魔丸，灵珠投胎为人，助周伐纣时可堪大用；而魔丸则会诞出魔王，为祸人间。元始天尊启动了天劫咒语，3年后天雷将会降临，摧毁魔丸。太乙受命将灵珠托生于陈塘关李靖家的儿子哪吒身上。然而阴差阳错，灵珠和魔丸竟然被掉包。本应是灵珠英雄的哪吒却成了混世大魔王。调皮捣蛋顽劣不堪的哪吒却徒有一颗做英雄的心。然而面对众人对魔丸的误解和即将来临的天雷的降临，哪吒是否命中注定会立地成魔？他将何去何从？\"\n",
    "text_2 = \"织女因擅离职守，堕入凡尘，遭星宿反噬，连累牛郎与孩子。多年后，被带往神界的织女后人金风，为替母赎罪而重返人间，收回星宿。途中，他意外结识渴望去往神界寻母的女孩小凡（玉露）。在结伴寻星的历险途中，阴差阳错知晓了织女罪案的真相……\"\n",
    "text_3 = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "text_list = [text_1, text_2, text_3]\n",
    "\n",
    "# 以文本存储\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    text_list,\n",
    "    embedding=embeddings2,\n",
    ")\n",
    "\n",
    "# 以langchain document存储\n",
    "# vector_store = InMemoryVectorStore(embedding=embeddings)\n",
    "# vector_store.add_documents(documents=split_docs)\n",
    "\n",
    "# vector_store = Chroma.from_texts(\n",
    "#     texts=docs,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=\"./chroma_db\",\n",
    "# )\n",
    "\n",
    "# vector_db = Chroma.from_texts(\n",
    "#     texts=splited_text,\n",
    "#     embedding=embeddings,\n",
    "#     persist_directory=\"./word_vector_db\"\n",
    "# )\n",
    "# vector_db.persist()\n",
    "\n",
    "\n",
    "# 向量检索器\n",
    "# # Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 1, \"fetch_k\": 2, \"lambda_mult\": 0.5},\n",
    ")\n",
    "user_query = \"请问Langchain是什么\"\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(user_query)\n",
    "# show the retrieved document's content\n",
    "print(retrieved_documents[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下为代调试代码\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.调用嵌入模型\n",
    "\n",
    "# 方式1. 通过OpenAI模式调用百炼embedding模型\n",
    "# import os\n",
    "# import getpass\n",
    "# from openai import OpenAI\n",
    "# if \"OPENAI_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "#     base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "# )\n",
    "\n",
    "# completion = client.embeddings.create(\n",
    "#     model=\"text-embedding-v3\",\n",
    "#     input='The clothes are of good quality and look good, definitely worth the wait. I love them.',\n",
    "#     dimensions=1024,\n",
    "#     encoding_format=\"float\"\n",
    "# )\n",
    "\n",
    "# print(completion.model_dump_json())\n",
    "\n",
    "# 方式2. 调用百炼embedding模型\n",
    "# pip install langchain-community\n",
    "# pip install dashscope\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v2\",\n",
    "    dashscope_api_key=\"sk-a5c5eb662fa64bb0b50e25765808d9f1\"\n",
    ")\n",
    "\n",
    "# 嵌入查询\n",
    "# text = \"This is a test document.\"\n",
    "# query_result = embeddings.embed_query(text)\n",
    "# print(\"文本向量长度：\", len(query_result), sep='')\n",
    "\n",
    "# 嵌入文档\n",
    "# doc_results = embeddings.embed_documents(\n",
    "#     [\n",
    "#         \"Hi there!\",\n",
    "#         \"Oh, hello!\",\n",
    "#         \"What's your name?\",\n",
    "#         \"My friends call me World\",\n",
    "#         \"Hello World!\"\n",
    "#     ])\n",
    "# print(\"文本向量数量：\", len(doc_results), \"，文本向量长度：\", len(doc_results[0]), sep='')\n",
    "\n",
    "\n",
    "# 3.存入向量数据库\n",
    "\n",
    "# 方式1. 存入内存\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())\n",
    "vector_store = InMemoryVectorStore(embedding=embeddings)\n",
    "document_1 = Document(id=\"1\", page_content=\"foo\", metadata={\"baz\": \"bar\"})\n",
    "document_2 = Document(id=\"2\", page_content=\"thud\", metadata={\"bar\": \"baz\"})\n",
    "document_3 = Document(id=\"3\", page_content=\"i will be deleted :(\")\n",
    "documents = [document_1, document_2, document_3]\n",
    "print(documents)\n",
    "vector_store.add_documents(documents=documents)\n",
    "\n",
    "# top_n = 10\n",
    "# for index, (id, doc) in enumerate(vector_store.store.items()):\n",
    "#     if index < top_n:\n",
    "#         # docs have keys 'id', 'vector', 'text', 'metadata'\n",
    "#         print(f\"{id}: {doc['text']}\")\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# 相似性检索\n",
    "# results = vector_store.similarity_search(query=\"thud\",k=1)\n",
    "# for doc in results:\n",
    "#     print(f\"* {doc.page_content} [{doc.metadata}]\")\n",
    "\n",
    "# retriever方式检索\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 1, \"fetch_k\": 2, \"lambda_mult\": 0.5},\n",
    ")\n",
    "retriever.invoke(\"thud\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
